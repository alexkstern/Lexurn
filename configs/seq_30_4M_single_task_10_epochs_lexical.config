[model]
vocab_size=4
d_model=256
n_layers=4
n_heads=4
context_len=30
dropout_rate=0.05
mlp_expansion=4

[training]
learning_rate=1e-4
batch_size=32
num_epochs=10
optimizer=adamw
weight_decay=0.01
warmup_steps=300
max_grad_norm=1.0
wandb=False
early_stopping=False
early_stopping_patience=10
early_stopping_min_delta=1e-4

[dataset]
n_tasks=1
n_steps=100000
seed=43

[evaluation]
eval_frequency=100
save_frequency=1000
test_steps=256
report_evaluations=True

[experiment]
config_name=single_task_lexical
save_results=True
model_type=lexical

[system]
device=auto