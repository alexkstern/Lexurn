[model]
vocab_size=4
d_model=128
n_layers=4
n_heads=4
context_len=8
dropout_rate=0.05
mlp_expansion=4

[training]
learning_rate=1e-5
batch_size=64
num_epochs=1
optimizer=adamw
weight_decay=0.01
warmup_steps=1000
max_grad_norm=1.0
wandb=True

[dataset]
n_tasks=16
n_steps=50000
seed=42

[evaluation]
eval_frequency=100
save_frequency=1000
test_steps=256
report_evaluations=True

[experiment]
config_name=multi_task_experiment
save_results=True

[system]
device=auto