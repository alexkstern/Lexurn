[model]
vocab_size=4
d_model=128
n_layers=4
n_heads=4
context_len=8
dropout_rate=0.05
mlp_expansion=4

[lexical_invariance]
lex_invariance=False
freeze_embeddings=False

[training]
learning_rate=1e-4
batch_size=64
num_epochs=100
optimizer=adamw
weight_decay=0.01
warmup_steps=1000
max_grad_norm=1.0
wandb=True

[dataset]
n_tasks=32
n_steps=10000
seed=42

[evaluation]
eval_frequency=100
save_frequency=1000
test_steps=256
report_evaluations=True

[experiment]
config_name=dummy
save_results=true

[system]
device=auto

