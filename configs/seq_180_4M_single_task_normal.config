[model]
vocab_size=4
d_model=256
n_layers=4
n_heads=4
context_len=180
dropout_rate=0.05
mlp_expansion=4

[training]
learning_rate=1e-4
batch_size=128
num_epochs=1
optimizer=adamw
weight_decay=0.01
warmup_steps=300
max_grad_norm=1.0
wandb=True
early_stopping=True
early_stopping_patience=2
early_stopping_min_delta=1e-4

[dataset]
n_tasks=1
n_steps=1300000
seed=42

[evaluation]
eval_frequency=100
save_frequency=1000
test_steps=256
report_evaluations=True

[experiment]
config_name=single_task_normal
save_results=True
model_type=normal

[system]
device=auto