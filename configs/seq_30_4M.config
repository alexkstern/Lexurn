[model]
vocab_size=4
d_model=256
n_layers=4
n_heads=4
context_len=30
dropout_rate=0.05
mlp_expansion=4

[training]
learning_rate=1e-4
batch_size=32
num_epochs=1
optimizer=adamw
weight_decay=0.01
warmup_steps=300
max_grad_norm=1.0
wandb=False

[dataset]
n_tasks=1
n_steps=300000
seed=42

[evaluation]
eval_frequency=100
save_frequency=1000
test_steps=256
report_evaluations=True

[experiment]
config_name=single_task_experiment
save_results=True

[system]
device=auto